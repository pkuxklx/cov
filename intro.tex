Covariance matrix estimation is an important problem in statistics
and econometrics. When the dimensionality $N$ of the random vector $\boldsymbol{X}_t=(X_{1t},\dots,X_{Nt})^{\intercal}$ under inspection is large, estimating its covariance matrix is challenging. It is well known that the sample covariance matrix is ill-conditioned when the dimension exceeds the sample size. In that case, some structures need to be imposed on the covariance matrix, and regularization techniques need to be applied for consistent estimation. 

In the era of big data, we are gaining access to more and more auxiliary information apart from the observations of $\{\boldsymbol{X}_t\}_{t=1}^T$, which could potentially help us learn about the underlying structure of the covariance matrix (i.e., connectivity among entities). Consider the case of equity return covariance. \cite{israelsen2016does} finds that stocks covered by similar sets of analysts co-move a lot. \cite{ge2022news} find that stocks co-mentioned in business news exhibit excess co-movement beyond risk factors. Apply textual analysis to firms' 10-K reports, \cite{hoberg2016text} define peer groups within which firms are fundamentally similar. All these aforementioned auxiliary network information could help us to learn about the connectivity among the stocks. However, the current literature either completely ignores this kind of auxiliary information or uses part of them under some very restrictive settings. 

In this paper, we incorporate auxiliary information about connectivity among entities (i.e., network information) into the estimation of large covariance matrices. Depending on the features of the auxiliary network information at hand and the structure of the covariance matrix, we provide separate avenues for application and derive their theories accordingly. 

The first method we propose is called Network Guided Thresholding. The method is applicable when auxiliary information identifies the location of “significant” elements in the covariance matrix while staying silent about the relative importance of neighbors for each node. For instance, industry information is a good example of such auxiliary information as it implies a block-diagonal network where every node is equal within an industry. The original series of thresholding methods (\cite{bickel2008CovarianceRegularization}, \cite{cai2011adaptive}, \cite{fan2013large}) keep the “significant” elements in the sample covariance and shrink the rest based on statistical information only under the assumption of sparsity (or conditional sparsity). These thresholding estimators require no knowledge about the location information. We, on the other hand, utilize auxiliary network information to identify the location of these “significant” elements. We keep the “significant” elements in the sample covariance and then apply generalized thresholding to the rest. The work closest to this method is \cite{fan2016incorporating}, where the authors apply location-based thresholding utilizing sector information. However, the factor model residual correlation structure is not as simple as a block diagonal assumed by \cite{fan2016incorporating}, and our method accommodates more complex structures. We derive the theoretical properties of the Network Guided Thresholding estimator. Compared with \cite{bickel2008CovarianceRegularization}, we consider a larger class of sparse covariance matrices as we distinguish "large" and "small" elements using the auxiliary information and we quantify their behaviors separately.
We show the consistency of the estimator in the operator norm as $(log N)/T\to 0$, uniformly over the class of matrices that satisfy our sparsity condition. And we also show that the Network Guided Thresholding estimator achieves optimal rate as in \cite{bickel2008CovarianceRegularization} over a larger parameter space. Simulation results show that its finite sample performance is better than all other pure statistical methods as long as the auxiliary network information is decent (not too much type I and type II errors). In particular, type I error hurts the performance of the Network Guided Thresholding Estimator more than type II error does.

The second method we propose is called Network Guided Banding. \cite{bickel2008RegularizedEstimation} show that uniformly over the class of the "approximately bandable" matrices, the banding estimator shows a superior convergence rate. However, according to their definition, elements become smaller in magnitude as one moves away from the diagonal. This definition is appropriate for applications with natural orderings of variables, such as time series, climatology, and spectroscopy. However, in most cases, such orderings do not exist, which means the banding estimator cannot be applied. In this paper, we propose a theoretical framework that expands the class of "bandable" matrices, making this method applicable to a wider range of scenarios. One of the key features of this new method is that it is permutation-invariant, while the original banding estimator performs poorly on a permutated ordering. Different from the first method, we will need our auxiliary information to reveal the relative importance of neighbors for each node for this method to be applicable. For example, the analyst-coverage (\cite{israelsen2016does}), news co-mentioning (\cite{ge2021dynamic}), and text-based product similarity (\cite{hoberg2016text}) methods all provide degrees of similarities among entities, according to which we could rank the relative importance of neighbors for each node. Taking news co-mentioning for illustration, firms co-mentioned by the same piece of news are treated as linked, and the frequency of co-mentioning could be used to measure the strength of linkages and thus rank their relative importance. We derive the theoretical properties of the Network Guided Banding estimator. We show the consistency of the estimator in the operator norm and Frobenius norm as $(log N)/T\to 0$, uniformly over the class of matrices that satisfy our sparsity and "bandable" condition.  And we also show that the Network Guided Banding estimator achieves optimal rate as in \cite{bickel2008RegularizedEstimation} over a larger parameter space. Simulation results show that its finite sample performance is excellent, and it outperforms all other pure statistical methods as long as the auxiliary network information has decent quality (the proportion of most important neighbors identified using the auxiliary network information cannot be too low).

A growing number of works have been proposed in the literature to study the covariance matrix estimation when the dimensionality is large.
\cite{bickel2008covariance} develops the theory for universal thresholding, which assumes the diagonal of the covariance matrix is uniformly bounded. 
\cite{cai2011AdaptiveThresholding} relaxes the uniform boundedness assumption and proposes an adaptive thresholding estimator where there are entry-adaptive thresholds. \cite{fan2013large} argues that common factors should be extracted first before applying thresholding when there are ”extremely spiked” eigenvalues in the covariance and such a covariance matrix is conditionally sparse. Another strand of literature has tried to correct the spectrum of the sample covariance matrix instead of imposing sparsity on the elements of the matrix. For instance, \cite{ledoit2004HoneyShrunk} and \cite{ledoit2012nonlinear} have proposed linear and nonlinear shrinkage estimators which apply shrinkage to the eigenvalues of the sample covariance matrix. The linear shrinkage does this by finding the linear combination of the sample covariance and a well-conditioned matrix, such as the identity matrix. And the nonlinear shrinkage estimator corrects the eigenvalues using the asymptotic Marchenko–Pastur distribution. One of the advantages of shrinkage estimators is
that they are well-conditioned, while the estimators based on sparsity often require choosing tuning parameters to guarantee positive definiteness. However, shrinkage estimators may be undesirable when the true covariance matrix is sparse. These aforementioned methods all completely ignore the location information implied by auxiliary information that might be out there and rely on observations of $\{\boldsymbol{X}_t\}_{t=1}^T$ only. The literature also embraces the usage of very simple location information. \cite{bickel2008regularized} proposes the banding and tapering estimators, where indexes have orderings and elements in the covariance matrix become smaller in magnitude as one moves away from the diagonal. They show that the banding estimator has a superior convergence rate by utilizing the location information. However, the underlying structure of these "bandable" matrices is very restrictive, which leaves the banding estimator inapplicable in most scenarios. The novelty of this paper is that we augment the estimation of large covariance matrices with auxiliary network information. Depending on the features of the auxiliary network information at hand, we provide two separate avenues for application. We derive their theories accordingly, and we show that both Network Guided estimators have good theoretical and numerical properties.


In our empirical application to the estimation of covariance of asset returns, the first candidate is the news-implied network. Same as \cite{ge2022news}, we use news data from RavenPack Equity files Dow Jones Edition from the beginning of 2004 to the end of 2015. This comprehensive news dataset combines relevant content from multiple sources, including Dow Jones Newswires, Wall Street Journal, and Barron’s  MarketWatch, which produce the most actively monitored streams of news articles in the financial system. We identify linkages among firms by news co-mentioning. The second candidate network is Institutional Brokers Estimate System (IBES) analyst co-coverage network. \cite{israelsen2016does} documents that stocks linked by analysts exhibit excess co-movement. To construct the analyst co-coverage-based adjacency matrix, we use the Institutional Brokers Estimate System (IBES) detail history files.  We then use the estimated covariance matrices to construct the global minimum variance portfolio.

Although in this paper, we are applying augment network information to the estimation of large static covariance matrices, a similar idea can be extended to the estimation of large dynamic covariance matrices. For example, dynamic network information could be well incorporated into the conditioning information set in \cite{chen2019new}.


The rest of the paper is organized as follows. In Section 2, we introduce the Network Guided Thresholding estimator and Network Guided Banding estimator. We lay down the assumptions and derive the theoretical results. Section 3 gives simulations comparing the performance of our proposed estimators and alternative estimators. In Section 4, we apply the methods empirically to estimate the covariance matrix of stock returns, and we provide a portfolio study. We conclude and briefly discuss future work in Section 5.